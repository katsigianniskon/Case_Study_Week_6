{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbae957-bdd5-44f6-91c8-10da1cd86d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TASK 1: MODEL IMPLEMENTATION FOR PORTFOLIO OPTIMIZATION\n",
    "# Goal: Predict portfolio returns for Sortino/Sharpe optimization\n",
    "# Realistic expectation: R¬≤ ~ 0.05-0.15 is GOOD in finance!\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "DATA_DIR = 'data_week6'\n",
    "TICKERS = ['INTC', 'NVDA', 'AMD', 'QCOM', 'TXN', 'MU', 'AVGO', 'AMAT', 'ASML', 'TSM']\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "FORWARD_DAYS = 10  # 10-day forward returns (balance between noise and signal)\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"ü§ñ TASK 1: TREE-BASED MODELS FOR PORTFOLIO OPTIMIZATION\")\n",
    "print(\"=\"*90)\n",
    "print(f\"üìä Target: Equal-weight portfolio {FORWARD_DAYS}-day returns\")\n",
    "print(f\"üìä Models: Random Forest, Gradient Boosting, XGBoost\")\n",
    "print(f\"üìä Objective: Returns prediction for Sortino/Sharpe optimization\")\n",
    "print(f\"‚ö†Ô∏è  Note: In finance, R¬≤ > 0.05 is considered good!\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüì• Loading data...\")\n",
    "\n",
    "prices_df = pd.read_csv(f'{DATA_DIR}/stock_prices.csv', index_col='Date', parse_dates=True)\n",
    "returns_df = pd.read_csv(f'{DATA_DIR}/stock_returns.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "print(f\"‚úÖ Loaded: {returns_df.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING - WEEK 5 + KEY PREDICTORS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîß Engineering features (Week 5 + portfolio-focused)...\")\n",
    "\n",
    "features_df = pd.DataFrame(index=returns_df.index)\n",
    "\n",
    "# --- WEEK 5 FEATURES ---\n",
    "print(\"  üìç Week 5 Features: HMact, Herd_t, VRSpike\")\n",
    "\n",
    "# HMact (per-asset)\n",
    "for ticker in TICKERS:\n",
    "    features_df[f'{ticker}_HMact'] = returns_df[ticker].abs().rolling(10).sum()\n",
    "\n",
    "# Herd_t (cross-sectional)\n",
    "herd_mat = np.sign(returns_df[TICKERS])\n",
    "features_df['Herd_t'] = herd_mat.mean(axis=1)\n",
    "\n",
    "# VRSpike (per-asset)\n",
    "for ticker in TICKERS:\n",
    "    sigma5 = returns_df[ticker].rolling(5).std()\n",
    "    sigma20 = returns_df[ticker].rolling(20).std()\n",
    "    features_df[f'{ticker}_VRSpike'] = sigma5 / (sigma20 + 1e-10)\n",
    "\n",
    "# --- PORTFOLIO-LEVEL FEATURES (CRITICAL FOR OPTIMIZATION) ---\n",
    "print(\"  üìç Portfolio-Level Features\")\n",
    "\n",
    "# Portfolio momentum (multiple horizons)\n",
    "portfolio_ret = returns_df[TICKERS].mean(axis=1)\n",
    "features_df['portfolio_momentum_5d'] = portfolio_ret.rolling(5).sum()\n",
    "features_df['portfolio_momentum_10d'] = portfolio_ret.rolling(10).sum()\n",
    "features_df['portfolio_momentum_20d'] = portfolio_ret.rolling(20).sum()\n",
    "\n",
    "# Portfolio volatility (for Sortino calculation later)\n",
    "features_df['portfolio_volatility_20d'] = portfolio_ret.rolling(20).std()\n",
    "features_df['portfolio_volatility_60d'] = portfolio_ret.rolling(60).std()\n",
    "\n",
    "# Downside volatility (CRITICAL for Sortino ratio)\n",
    "negative_rets = portfolio_ret.apply(lambda x: x if x < 0 else 0)\n",
    "features_df['portfolio_downside_vol_20d'] = negative_rets.rolling(20).std()\n",
    "\n",
    "# Cross-sectional dispersion\n",
    "features_df['cross_sectional_vol'] = returns_df[TICKERS].std(axis=1)\n",
    "features_df['max_min_spread'] = returns_df[TICKERS].max(axis=1) - returns_df[TICKERS].min(axis=1)\n",
    "\n",
    "# Correlation (diversification proxy)\n",
    "features_df['avg_correlation_20d'] = returns_df[TICKERS].rolling(20).corr().groupby(level=0).mean().mean(axis=1)\n",
    "\n",
    "# --- PER-STOCK AGGREGATES (REDUCED SET) ---\n",
    "print(\"  üìç Per-Stock Key Features\")\n",
    "\n",
    "# Average momentum across stocks\n",
    "for window in [10, 20]:\n",
    "    momentum_cols = [returns_df[t].rolling(window).sum() for t in TICKERS]\n",
    "    features_df[f'avg_stock_momentum_{window}d'] = pd.concat(momentum_cols, axis=1).mean(axis=1)\n",
    "\n",
    "# Average volatility across stocks\n",
    "for window in [20, 60]:\n",
    "    vol_cols = [returns_df[t].rolling(window).std() for t in TICKERS]\n",
    "    features_df[f'avg_stock_volatility_{window}d'] = pd.concat(vol_cols, axis=1).mean(axis=1)\n",
    "\n",
    "# --- TARGET: Forward portfolio return ---\n",
    "print(f\"  üìç Target: {FORWARD_DAYS}-day forward portfolio return\")\n",
    "features_df['target'] = portfolio_ret.rolling(FORWARD_DAYS).sum().shift(-FORWARD_DAYS)\n",
    "\n",
    "# --- CLEAN ---\n",
    "print(\"\\nüßπ Cleaning data...\")\n",
    "features_df = features_df.replace([np.inf, -np.inf], np.nan)\n",
    "features_df = features_df.dropna()\n",
    "\n",
    "num_features = len(features_df.columns) - 1\n",
    "print(f\"‚úÖ Features: {num_features}\")\n",
    "print(f\"‚úÖ Samples: {len(features_df)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN-TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Train-test split (80/20)...\")\n",
    "\n",
    "X = features_df.drop('target', axis=1)\n",
    "y = features_df['target']\n",
    "\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"‚úÖ Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODELS (BALANCED REGULARIZATION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nü§ñ Defining models...\")\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=150,\n",
    "        max_depth=6,\n",
    "        min_samples_split=30,\n",
    "        min_samples_leaf=15,\n",
    "        max_features='sqrt',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=150,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        min_samples_split=30,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=150,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Defined {len(models)} models\")\n",
    "\n",
    "# ============================================================================\n",
    "# CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüîÑ {N_FOLDS}-fold cross-validation...\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=N_FOLDS)\n",
    "cv_results = {name: {'cv_scores': [], 'train_r2': None, 'test_r2': None, \n",
    "                      'train_mse': None, 'test_mse': None, 'train_mae': None, 'test_mae': None} \n",
    "              for name in models.keys()}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n  üîπ {name}...\")\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model_clone = type(model)(**model.get_params())\n",
    "        model_clone.fit(X_tr, y_tr)\n",
    "        \n",
    "        y_val_pred = model_clone.predict(X_val)\n",
    "        r2 = r2_score(y_val, y_val_pred)\n",
    "        fold_scores.append(r2)\n",
    "        \n",
    "        print(f\"     Fold {fold}: R¬≤ = {r2:.4f}\")\n",
    "    \n",
    "    cv_results[name]['cv_scores'] = fold_scores\n",
    "    print(f\"     CV Mean R¬≤: {np.mean(fold_scores):.4f} ¬± {np.std(fold_scores):.4f}\")\n",
    "    \n",
    "    # Final model\n",
    "    print(f\"     Training final model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    cv_results[name]['train_r2'] = r2_score(y_train, y_train_pred)\n",
    "    cv_results[name]['test_r2'] = r2_score(y_test, y_test_pred)\n",
    "    cv_results[name]['train_mse'] = mean_squared_error(y_train, y_train_pred)\n",
    "    cv_results[name]['test_mse'] = mean_squared_error(y_test, y_test_pred)\n",
    "    cv_results[name]['train_mae'] = mean_absolute_error(y_train, y_train_pred)\n",
    "    cv_results[name]['test_mae'] = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"     ‚úÖ Train R¬≤: {cv_results[name]['train_r2']:.4f}, Test R¬≤: {cv_results[name]['test_r2']:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PAIRED T-TESTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Paired t-tests...\")\n",
    "\n",
    "model_names = list(models.keys())\n",
    "p_values = {}\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i + 1, len(model_names)):\n",
    "        name1, name2 = model_names[i], model_names[j]\n",
    "        scores1 = cv_results[name1]['cv_scores']\n",
    "        scores2 = cv_results[name2]['cv_scores']\n",
    "        \n",
    "        t_stat, p_val = stats.ttest_rel(scores1, scores2)\n",
    "        p_values[f\"{name1} vs {name2}\"] = p_val\n",
    "        \n",
    "        sig = \"‚úì Significant\" if p_val < 0.05 else \"‚úó Not significant\"\n",
    "        print(f\"  ‚Ä¢ {name1} vs {name2}: p = {p_val:.4f} ({sig})\")\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üìä TASK 1 RESULTS - PORTFOLIO OPTIMIZATION\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "results_data = []\n",
    "for name in model_names:\n",
    "    if name == 'Random Forest':\n",
    "        p_val = '‚Äî'\n",
    "    else:\n",
    "        p_val = p_values.get(f\"Random Forest vs {name}\", '‚Äî')\n",
    "        if isinstance(p_val, float):\n",
    "            p_val = f\"{p_val:.4f}\"\n",
    "    \n",
    "    results_data.append({\n",
    "        'Model': name,\n",
    "        'Train R¬≤': f\"{cv_results[name]['train_r2']:.4f}\",\n",
    "        'Test R¬≤': f\"{cv_results[name]['test_r2']:.4f}\",\n",
    "        'Test MSE': f\"{cv_results[name]['test_mse']:.6f}\",\n",
    "        'Test MAE': f\"{cv_results[name]['test_mae']:.6f}\",\n",
    "        'p-value vs RF': p_val\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nüìà INTERPRETATION:\")\n",
    "best_model = max(cv_results.items(), key=lambda x: x[1]['test_r2'])[0]\n",
    "best_r2 = cv_results[best_model]['test_r2']\n",
    "\n",
    "if best_r2 > 0.05:\n",
    "    print(f\"‚úÖ Best model ({best_model}) achieves Test R¬≤ = {best_r2:.4f}\")\n",
    "    print(\"‚úÖ This is GOOD for financial prediction! (R¬≤ > 0.05)\")\n",
    "    print(\"‚úÖ Models are ready for portfolio optimization (Sortino/Sharpe)\")\n",
    "elif best_r2 > 0:\n",
    "    print(f\"‚ö†Ô∏è  Best model ({best_model}) achieves Test R¬≤ = {best_r2:.4f}\")\n",
    "    print(\"‚ö†Ô∏è  Low but positive - models have some predictive power\")\n",
    "    print(\"‚úÖ Can proceed with portfolio optimization cautiously\")\n",
    "else:\n",
    "    print(\"‚ùå Models show no predictive power on test set\")\n",
    "    print(\"‚ö†Ô∏è  Consider: longer horizons, different features, or ensemble\")\n",
    "\n",
    "print(\"\\nüíæ Saving results...\")\n",
    "output_file = f'{DATA_DIR}/task1_results_portfolio.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Saved: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"‚úÖ TASK 1 COMPLETE - Ready for Portfolio Optimization!\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1: RESULTS VISUALIZATION ONLY\n",
    "# Add this code at the end of your Task 1 script\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nüìä Creating visualization...\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE 4-PANEL FIGURE\n",
    "# ============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Define colors and model names\n",
    "colors = ['#66c2a5', '#fc8d62', '#8da0cb']  # Green, Orange, Blue\n",
    "model_names = ['Random Forest', 'Gradient Boosting', 'XGBoost']\n",
    "\n",
    "# ============================================================================\n",
    "# SUBPLOT 1: Test Set R¬≤ (Top Left)\n",
    "# ============================================================================\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "r2_values = [cv_results[name]['test_r2'] for name in model_names]\n",
    "bars1 = ax1.bar(model_names, r2_values, color=colors, alpha=0.8, \n",
    "                edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on top\n",
    "for bar, val in zip(bars1, r2_values):\n",
    "    height = bar.get_height()\n",
    "    y_pos = height + 0.001 if height > 0 else height - 0.001\n",
    "    va = 'bottom' if height > 0 else 'top'\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., y_pos,\n",
    "             f'{val:.4f}', ha='center', va=va, fontsize=10, fontweight='bold')\n",
    "\n",
    "ax1.set_ylabel('R¬≤', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Test Set R¬≤', fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.set_ylim(min(r2_values) - 0.01, max(r2_values) + 0.005)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax1.set_axisbelow(True)\n",
    "\n",
    "# ============================================================================\n",
    "# SUBPLOT 2: Test Set MSE (Top Right)\n",
    "# ============================================================================\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "mse_values = [cv_results[name]['test_mse'] for name in model_names]\n",
    "bars2 = ax2.bar(model_names, mse_values, color=colors, alpha=0.8,\n",
    "                edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars2, mse_values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.00002,\n",
    "             f'{val:.6f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax2.set_ylabel('MSE', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Test Set MSE', fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax2.set_axisbelow(True)\n",
    "\n",
    "# ============================================================================\n",
    "# SUBPLOT 3: Predicted vs Actual (Bottom Left) - Random Forest\n",
    "# ============================================================================\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "# Get Random Forest predictions\n",
    "rf_model = models['Random Forest']\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_r2 = cv_results['Random Forest']['test_r2']\n",
    "\n",
    "# Scatter plot\n",
    "ax3.scatter(y_test, rf_pred, alpha=0.5, s=50, color='#66c2a5', \n",
    "            edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Perfect prediction line\n",
    "min_val = min(y_test.min(), rf_pred.min())\n",
    "max_val = max(y_test.max(), rf_pred.max())\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, \n",
    "         label='Perfect Prediction', alpha=0.7)\n",
    "\n",
    "ax3.set_xlabel('Actual 10-Day Return', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Predicted 10-Day Return', fontsize=12, fontweight='bold')\n",
    "ax3.set_title(f\"Random Forest - R¬≤={rf_r2:.4f}\", fontsize=14, fontweight='bold', pad=15)\n",
    "ax3.legend(loc='upper left', fontsize=10)\n",
    "ax3.grid(alpha=0.3, linestyle='--')\n",
    "ax3.set_axisbelow(True)\n",
    "\n",
    "# ============================================================================\n",
    "# SUBPLOT 4: Top 15 Features - Random Forest (Bottom Right)\n",
    "# ============================================================================\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "# Reverse order for horizontal bar plot (highest at top)\n",
    "feature_importance = feature_importance.iloc[::-1]\n",
    "\n",
    "# Create horizontal bar plot with proper alignment\n",
    "y_pos = np.arange(len(feature_importance))\n",
    "bars4 = ax4.barh(y_pos, \n",
    "                 feature_importance['importance'].values,\n",
    "                 color='#8da0cb', alpha=0.8, edgecolor='black', \n",
    "                 linewidth=1, height=0.7, align='center')\n",
    "\n",
    "# Set y-axis with proper alignment\n",
    "ax4.set_yticks(y_pos)\n",
    "ax4.set_yticklabels(feature_importance['feature'].values, fontsize=9)\n",
    "ax4.set_ylim(-0.5, len(feature_importance) - 0.5)  # Proper bounds\n",
    "\n",
    "# Labels and title\n",
    "ax4.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Top 15 Features - Random Forest', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "# Grid aligned with bars\n",
    "ax4.grid(axis='x', alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "ax4.set_axisbelow(True)\n",
    "\n",
    "# Adjust margins\n",
    "ax4.margins(y=0.01)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TITLE & SAVE\n",
    "# ============================================================================\n",
    "\n",
    "fig.suptitle('Task 1: Portfolio Return Prediction (10-Day Forward Returns)', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "\n",
    "# Save\n",
    "output_file = f'{DATA_DIR}/task1_results_visualization.png'\n",
    "plt.savefig(output_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"‚úÖ Saved visualization: {output_file}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"‚úÖ VISUALIZATION COMPLETE!\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0876186-8a78-4a58-83ce-b1e6e9f2e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TASK 3: FEATURE IMPORTANCE & SHAP ANALYSIS\n",
    "# Best Model: Random Forest (from Task 1)\n",
    "# Requirements: Built-in importance, SHAP values, comparison plots, analysis\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"üìä TASK 3: FEATURE IMPORTANCE & SHAP ANALYSIS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# ============================================================================\n",
    "# PREREQUISITE: Ensure Task 1 objects exist\n",
    "# ============================================================================\n",
    "# Required objects from Task 1:\n",
    "# - models (dict with trained models)\n",
    "# - cv_results (dict with results)\n",
    "# - X_train, X_test, y_test\n",
    "# - DATA_DIR\n",
    "\n",
    "# Select best model (Random Forest from Task 1)\n",
    "best_model_name = 'Random Forest'\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nüéØ Selected Model: {best_model_name}\")\n",
    "print(f\"   Test R¬≤: {cv_results[best_model_name]['test_r2']:.4f}\")\n",
    "print(f\"   Test MSE: {cv_results[best_model_name]['test_mse']:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: EXTRACT BUILT-IN FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 1: Extracting built-in feature importance...\")\n",
    "\n",
    "# Get feature importances from Random Forest\n",
    "feature_names = X_train.columns.tolist()\n",
    "importances = best_model.feature_importances_\n",
    "\n",
    "# Create DataFrame\n",
    "builtin_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(builtin_importance)} feature importances\")\n",
    "print(\"\\nüìä Top 10 Features (Built-in Importance):\")\n",
    "print(builtin_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "builtin_file = f'{DATA_DIR}/task3_builtin_importance.csv'\n",
    "builtin_importance.to_csv(builtin_file, index=False)\n",
    "print(f\"üíæ Saved: {builtin_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: COMPUTE SHAP VALUES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 2: Computing SHAP values (this may take a moment)...\")\n",
    "\n",
    "# Initialize SHAP explainer for tree-based models\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "# Compute SHAP values for test set\n",
    "print(\"   Computing SHAP values for test set...\")\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Get expected value (baseline) - handle both scalar and array\n",
    "expected_value = explainer.expected_value\n",
    "if isinstance(expected_value, np.ndarray):\n",
    "    expected_value = expected_value[0] if len(expected_value) > 0 else expected_value.item()\n",
    "\n",
    "print(f\"‚úÖ SHAP values computed\")\n",
    "print(f\"   Shape: {shap_values.shape}\")\n",
    "print(f\"   Expected value (baseline): {expected_value:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: SHAP IMPORTANCE (Mean Absolute SHAP)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 3: Calculating SHAP-based importance...\")\n",
    "\n",
    "# Calculate mean absolute SHAP value for each feature\n",
    "shap_importance_values = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Create DataFrame\n",
    "shap_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'SHAP_Importance': shap_importance_values\n",
    "}).sort_values('SHAP_Importance', ascending=False)\n",
    "\n",
    "print(f\"‚úÖ SHAP importance calculated\")\n",
    "print(\"\\nüìä Top 10 Features (SHAP Importance):\")\n",
    "print(shap_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "shap_file = f'{DATA_DIR}/task3_shap_importance.csv'\n",
    "shap_importance.to_csv(shap_file, index=False)\n",
    "print(f\"üíæ Saved: {shap_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: CREATE SHAP SUMMARY PLOT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 4: Creating SHAP summary plot...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create SHAP summary plot (beeswarm)\n",
    "shap.summary_plot(\n",
    "    shap_values, \n",
    "    X_test, \n",
    "    feature_names=feature_names,\n",
    "    max_display=20,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "plt.title('SHAP Summary Plot - Feature Impact on Model Predictions', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('SHAP Value (impact on model output)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "summary_plot_file = f'{DATA_DIR}/task3_shap_summary_plot.png'\n",
    "plt.savefig(summary_plot_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"‚úÖ Saved: {summary_plot_file}\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: COMPARE BUILT-IN VS SHAP IMPORTANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 5: Creating comparison plots...\")\n",
    "\n",
    "# Merge both importance measures\n",
    "comparison_df = builtin_importance.merge(\n",
    "    shap_importance, \n",
    "    on='Feature', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Get top 20 features by SHAP importance\n",
    "top_features = comparison_df.nlargest(20, 'SHAP_Importance')\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# PLOT 1: Built-in Feature Importance (Top 20)\n",
    "ax1 = axes[0]\n",
    "top_builtin = top_features.sort_values('Importance')\n",
    "\n",
    "y_pos = np.arange(len(top_builtin))\n",
    "ax1.barh(y_pos, top_builtin['Importance'].values, \n",
    "         color='#66c2a5', alpha=0.8, edgecolor='black', linewidth=1)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(top_builtin['Feature'].values, fontsize=9)\n",
    "ax1.set_xlabel('Built-in Importance (MDI)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Built-in Feature Importance (Random Forest)', \n",
    "              fontsize=13, fontweight='bold', pad=15)\n",
    "ax1.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "ax1.set_axisbelow(True)\n",
    "\n",
    "# PLOT 2: SHAP Importance (Top 20)\n",
    "ax2 = axes[1]\n",
    "top_shap = top_features.sort_values('SHAP_Importance')\n",
    "\n",
    "y_pos = np.arange(len(top_shap))\n",
    "ax2.barh(y_pos, top_shap['SHAP_Importance'].values,\n",
    "         color='#fc8d62', alpha=0.8, edgecolor='black', linewidth=1)\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(top_shap['Feature'].values, fontsize=9)\n",
    "ax2.set_xlabel('SHAP Importance (Mean |SHAP|)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('SHAP-based Feature Importance', \n",
    "              fontsize=13, fontweight='bold', pad=15)\n",
    "ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "ax2.set_axisbelow(True)\n",
    "\n",
    "plt.suptitle('Comparison: Built-in vs SHAP Feature Importance (Top 20)', \n",
    "             fontsize=15, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "comparison_plot_file = f'{DATA_DIR}/task3_importance_comparison.png'\n",
    "plt.savefig(comparison_plot_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"‚úÖ Saved: {comparison_plot_file}\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: SCATTER PLOT - CORRELATION BETWEEN IMPORTANCES (FIXED FOR REAL)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 6: Creating correlation scatter plot...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(comparison_df['Importance'], \n",
    "           comparison_df['SHAP_Importance'],\n",
    "           s=100, alpha=0.6, edgecolors='black', linewidth=1, color='#8da0cb')\n",
    "\n",
    "# Get actual data ranges\n",
    "x_max = comparison_df['Importance'].max()\n",
    "y_max = comparison_df['SHAP_Importance'].max()\n",
    "\n",
    "# Add diagonal reference line through the data range\n",
    "# Use the MINIMUM of the two maxes to keep it visible\n",
    "diag_max = min(x_max, y_max)\n",
    "ax.plot([0, diag_max], [0, diag_max], 'r--', linewidth=2, alpha=0.5, label='Perfect Agreement')\n",
    "\n",
    "# Annotate top 5 features\n",
    "top5_features = comparison_df.nlargest(5, 'SHAP_Importance')\n",
    "for idx, row in top5_features.iterrows():\n",
    "    ax.annotate(row['Feature'], \n",
    "                (row['Importance'], row['SHAP_Importance']),\n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=9, alpha=0.9, fontweight='bold')\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = comparison_df[['Importance', 'SHAP_Importance']].corr().iloc[0, 1]\n",
    "\n",
    "# Set axis limits with small margins (NO ASPECT RATIO CONSTRAINT)\n",
    "ax.set_xlim(-0.002, x_max * 1.1)\n",
    "ax.set_ylim(-0.0002, y_max * 1.15)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Built-in Importance (MDI)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('SHAP Importance (Mean |SHAP|)', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Built-in vs SHAP Importance Correlation (r = {correlation:.3f})', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(fontsize=10, loc='upper left')\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# NO ax.set_aspect('equal') - this was the problem!\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "scatter_plot_file = f'{DATA_DIR}/task3_importance_correlation.png'\n",
    "plt.savefig(scatter_plot_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"‚úÖ Saved: {scatter_plot_file}\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: TOP 5 FEATURES DETAILED ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 7: Analyzing Top 5 Features...\")\n",
    "\n",
    "# Get top 5 by SHAP importance\n",
    "top5 = comparison_df.nlargest(5, 'SHAP_Importance')\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üèÜ TOP 5 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "for rank, (idx, row) in enumerate(top5.iterrows(), 1):\n",
    "    print(f\"\\n{rank}. {row['Feature']}\")\n",
    "    print(f\"   Built-in Importance: {row['Importance']:.6f}\")\n",
    "    print(f\"   SHAP Importance:     {row['SHAP_Importance']:.6f}\")\n",
    "\n",
    "# Save top 5 to CSV\n",
    "top5_file = f'{DATA_DIR}/task3_top5_features.csv'\n",
    "top5[['Feature', 'Importance', 'SHAP_Importance']].to_csv(top5_file, index=False)\n",
    "print(f\"\\nüíæ Saved: {top5_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: SHAP DEPENDENCE PLOTS FOR TOP 5\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 8: Creating SHAP dependence plots for Top 5 features...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (_, row) in enumerate(top5.iterrows()):\n",
    "    feature_name = row['Feature']\n",
    "    feature_idx = feature_names.index(feature_name)\n",
    "    \n",
    "    # Create dependence plot\n",
    "    shap.dependence_plot(\n",
    "        feature_idx,\n",
    "        shap_values,\n",
    "        X_test,\n",
    "        feature_names=feature_names,\n",
    "        ax=axes[idx],\n",
    "        show=False,\n",
    "        alpha=0.5\n",
    "    )\n",
    "    axes[idx].set_title(f'{feature_name}', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Remove extra subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('SHAP Dependence Plots - Top 5 Features (Non-linear Relationships)', \n",
    "             fontsize=15, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "dependence_plot_file = f'{DATA_DIR}/task3_shap_dependence_top5.png'\n",
    "plt.savefig(dependence_plot_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"‚úÖ Saved: {dependence_plot_file}\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üìà SUMMARY STATISTICS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(f\"\\nüìä Feature Importance Correlation:\")\n",
    "print(f\"   Pearson r = {correlation:.4f}\")\n",
    "print(f\"   Interpretation: {'Strong' if abs(correlation) > 0.7 else 'Moderate' if abs(correlation) > 0.4 else 'Weak'} agreement\")\n",
    "\n",
    "print(f\"\\nüìä Top 5 Features Account For:\")\n",
    "total_builtin = builtin_importance['Importance'].sum()\n",
    "total_shap = shap_importance['SHAP_Importance'].sum()\n",
    "top5_builtin_pct = (top5['Importance'].sum() / total_builtin) * 100\n",
    "top5_shap_pct = (top5['SHAP_Importance'].sum() / total_shap) * 100\n",
    "\n",
    "print(f\"   Built-in Importance: {top5_builtin_pct:.2f}%\")\n",
    "print(f\"   SHAP Importance:     {top5_shap_pct:.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL OUTPUT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"‚úÖ TASK 3 COMPLETE!\")\n",
    "print(\"=\"*90)\n",
    "print(\"\\nüìÅ Generated Files:\")\n",
    "print(f\"   1. {builtin_file}\")\n",
    "print(f\"   2. {shap_file}\")\n",
    "print(f\"   3. {summary_plot_file}\")\n",
    "print(f\"   4. {comparison_plot_file}\")\n",
    "print(f\"   5. {scatter_plot_file}\")\n",
    "print(f\"   6. {top5_file}\")\n",
    "print(f\"   7. {dependence_plot_file}\")\n",
    "print(\"=\"*90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
